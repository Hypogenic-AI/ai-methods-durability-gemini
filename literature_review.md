## Literature Review

### Research Area Overview
The durability of AI models is a critical area of research, focusing on how models maintain their performance and reliability over time. Key concepts include "model collapse," "concept drift," and "AI aging." These phenomena describe the degradation of model performance due to various factors, such as being trained on their own generated data or changes in the underlying data distribution.

### Key Papers

#### Paper 1: The curse of recursion: Training on generated data makes models forget
- **Authors**: Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson
- **Year**: 2023
- **Source**: arXiv:2305.17493
- **Key Contribution**: This paper introduces the concept of "model collapse," a degenerative process where models trained on data generated by other models forget the true underlying data distribution.
- **Methodology**: The authors demonstrate model collapse using Variational Autoencoders (VAEs), Gaussian Mixture Models (GMMs), and Large Language Models (LLMs). They show that over successive generations of training on generated data, the models lose information about the tails of the original data distribution and converge to a state with very small variance.
- **Datasets Used**: The paper uses artificially-generated Gaussians for the GMM and VAE experiments and the wikitext2 dataset for the LLM experiments.
- **Results**: The experiments show that model collapse is an inevitable outcome of training on generated data. The authors demonstrate that even with a small amount of original data preserved, the model's performance degrades significantly.
- **Code Available**: Not specified in the paper.
- **Relevance to Our Research**: This paper is highly relevant as it provides a theoretical and empirical foundation for understanding one of the key mechanisms of model degradation, which we are calling a lack of "durability."

#### Paper 2: Learning under Concept Drift: A Review
- **Authors**: Jie Lu, Anjin Liu, Fan Dong, Feng Gu, Joao Gama, and Guangquan Zhang
- **Year**: 2018
- **Source**: arXiv:1812.02892
- **Key Contribution**: This paper provides a comprehensive review of the field of concept drift, which is the phenomenon of the statistical properties of the target variable changing over time.
- **Methodology**: The paper categorizes concept drift research into three main areas: drift detection, drift understanding, and drift adaptation. It reviews over 130 papers and provides a framework for learning under concept drift.
- **Datasets Used**: The paper lists and discusses 10 popular synthetic datasets and 14 publicly available benchmark datasets for evaluating concept drift algorithms.
- **Results**: The paper summarizes the state-of-the-art in concept drift research and identifies several open research questions and future directions.
- **Code Available**: Not applicable (review paper).
- **Relevance to Our Research**: This paper is essential for our research as it provides a broad overview of the problem of model performance degradation. It will help us to identify relevant datasets, evaluation metrics, and baseline models for our experiments.

#### Paper 3: McUDI: A Model-Centric Unsupervised Degradation Indicator for AIOps Models
- **Authors**: Lorena Poenaru-Olaru, Luis Cruz, Jan Rellermeyer and Arie van Deursen
- **Year**: 2024
- **Source**: arXiv:2401.14093
- **Key Contribution**: This paper proposes McUDI, a model-centric unsupervised degradation indicator that detects when an AIOps model requires retraining.
- **Methodology**: McUDI uses feature importance ranking to select the most important features of a model and then applies the Kolmogorov-Smirnov (KS) statistical test to detect drift in the distribution of those features.
- **Datasets Used**: The paper uses two operational datasets: the Google Cluster Traces dataset and the Backblaze Disk Stats dataset.
- **Results**: The experiments show that McUDI can effectively detect model degradation and reduce the number of required retrainings and annotations compared to periodic retraining.
- **Code Available**: The authors publicly share their reproduction package on GitHub.
- **Relevance to Our Research**: This paper is highly relevant as it proposes a practical method for detecting model degradation in a real-world setting. McUDI could be a valuable tool for our experiments on measuring model durability.

### Common Methodologies
- **Model Collapse**: This is a degenerative process where models trained on generated data forget the true data distribution. This is often demonstrated using VAEs, GMMs, and LLMs.
- **Concept Drift Detection**: This involves detecting changes in the statistical properties of the target variable over time. This is often done by monitoring the model's performance or by using statistical tests to detect changes in the data distribution.
- **Unsupervised Degradation Indicators**: These are methods for detecting model degradation without the need for labeled data. McUDI is an example of this, which uses feature importance and statistical tests to detect drift.

### Standard Baselines
- **Periodic Retraining**: This is the most common baseline, where the model is retrained at regular intervals.
- **Static Model**: A model that is never retrained after its initial training.

### Evaluation Metrics
- **Drift Detection Accuracy**: This measures the ability of a drift detector to correctly identify drifts and non-drifts. This is often measured using metrics like Balanced Accuracy, Specificity, and Sensitivity.
- **Model Performance Preservation**: This measures how well a model maintains its performance over time. This is often measured using metrics like ROC AUC.
- **Label Costs**: This measures the number of labels required to retrain the model.

### Datasets in the Literature
- **wikitext2**: A language modeling dataset.
- **Google Cluster Traces**: A dataset of job traces from a Google cluster.
- **Backblaze Disk Stats**: A dataset of hard drive statistics.

### Gaps and Opportunities
- **Lack of a standardized benchmark**: There is no standardized benchmark for evaluating model durability. This makes it difficult to compare different methods.
- **Focus on specific types of drift**: Most research focuses on specific types of drift, such as concept drift or model collapse. There is a need for more research on how to handle multiple types of drift simultaneously.
- **Limited research on unsupervised methods**: Most research on model degradation detection relies on labeled data. There is a need for more research on unsupervised methods that do not require labeled data.

### Recommendations for Our Experiment
- **Recommended datasets**: The Google Cluster Traces and Backblaze Disk Stats datasets are good candidates for our experiments as they are publicly available and have been used in previous research on model degradation.
- **Recommended baselines**: We should use periodic retraining and a static model as baselines for our experiments.
- **Recommended metrics**: We should use a combination of drift detection accuracy, model performance preservation, and label costs to evaluate our methods.
- **Methodological considerations**: We should consider using a model-centric unsupervised degradation indicator like McUDI in our experiments. We should also consider how to handle the class imbalance in the Google Cluster Traces dataset.
