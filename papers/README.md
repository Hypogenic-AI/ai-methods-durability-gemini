# Downloaded Papers

1. [The curse of recursion: Training on generated data makes models forget](2305.17493_the_curse_of_recursion.pdf)
   - Authors: Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson
   - Year: 2023
   - arXiv: 2305.17493
   - Why relevant: This paper discusses "model collapse," a phenomenon where models trained on their own output forget the original data distribution. This is highly relevant to the concept of model durability and "AI aging."

2. [Learning under Concept Drift: A Review](2004.05785_learning_under_concept_drift.pdf)
   - Authors: Jie Lu, Anjin Liu, Fan Dong, Feng Gu, Joao Gama, and Guangquan Zhang
   - Year: 2018
   - arXiv: 1812.02892
   - Why relevant: This review paper provides a comprehensive overview of concept drift, which is a key reason for model performance degradation over time. It is essential for understanding why models become less "durable."

3. [McUDI: A Model-Centric Unsupervised Degradation Indicator for AIOps Models](2401.14093_mcudi.pdf)
   - Authors: Rongge Xiao, Marco Fabbri, Lisha Ermakova, and Kaixin Sui
   - Year: 2024
   - arXiv: 2401.14093
   - Why relevant: This paper proposes a method to detect model degradation without ground truth labels, which is a practical challenge in maintaining model durability. It directly addresses the "AI aging" problem.
